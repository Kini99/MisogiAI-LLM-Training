{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Generate Candidate Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to generate a set of answers for our prompts. We will use a base model for this. The following code will:\n",
    "\n",
    "1. Define 5 prompts.\n",
    "2. Load a pre-trained model and tokenizer (`distilgpt2`).\n",
    "3. Generate 4 candidate answers for each prompt.\n",
    "4. Save the prompts and answers to `answers.csv` with a placeholder `rank` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 1. Define prompts\n",
    "prompts = [\n",
    "    \"Tell me a joke about a programmer.\",\n",
    "    \"Summarize the plot of the movie Inception in one paragraph.\",\n",
    "    \"Write a mini-essay on the importance of recycling.\",\n",
    "    \"What is the difference between a fruit and a vegetable?\",\n",
    "    \"Explain the concept of machine learning to a 5-year-old.\"\n",
    "]\n",
    "\n",
    "# 2. Load model and tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set pad token if it's not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. Generate answers\n",
    "data = []\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Generate 4 answers\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        num_return_sequences=4,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "    \n",
    "    for i in range(4):\n",
    "        answer = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "        # Remove the prompt from the answer\n",
    "        answer = answer[len(prompt):].strip()\n",
    "        data.append({\n",
    "            'prompt': prompt,\n",
    "            'answer': answer,\n",
    "            'rank': 1  # Placeholder rank\n",
    "        })\n",
    "\n",
    "# 4. Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('q2_reward/answers.csv', index=False)\n",
    "\n",
    "print(\"Successfully generated and saved answers to `q2_reward/answers.csv`.\")\n",
    "print(\"Please manually edit `q2_reward/answers.csv` to rank the answers for each prompt from 1 (best) to 4 (worst).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Manually Rank the Answers\n",
    "\n",
    "Now, open the `q2_reward/answers.csv` file in a spreadsheet editor or a text editor. For each prompt, you will see 4 generated answers. Please evaluate them and assign a rank from 1 to 4, where 1 is the best answer and 4 is the worst. \n",
    "\n",
    "**Do not proceed until you have ranked all the answers.**\n",
    "\n",
    "After you have ranked the answers, you can train the reward model by running the `train.py` script from your terminal:\n",
    "```bash\n",
    "python q2_reward/train.py\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Evaluate the Reward Model\n",
    "\n",
    "Once the reward model is trained, we can use it to score answers. The following code will:\n",
    "\n",
    "1. Load the trained reward model and tokenizer from the `reward_model/` directory.\n",
    "2. Load the ranked answers from `answers.csv`.\n",
    "3. Calculate the reward score for each answer.\n",
    "4. Plot the reward scores against the manual ranks to see if they correlate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# 1. Load the reward model and tokenizer\n",
    "model_path = 'q2_reward/reward_model'\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Model directory `{model_path}` not found.\")\n",
    "    print(\"Please train the model first by running `python q2_reward/train.py`.\")\n",
    "else:\n",
    "    reward_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    reward_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    # 2. Load ranked answers\n",
    "    df = pd.read_csv('q2_reward/answers.csv')\n",
    "\n",
    "    # 3. Calculate reward scores\n",
    "    scores = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = f\"{row['prompt']} {row['answer']}\"\n",
    "        inputs = reward_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            score = reward_model(**inputs).logits[0].item()\n",
    "        scores.append(score)\n",
    "\n",
    "    df['score'] = scores\n",
    "\n",
    "    # 4. Plot the results\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    sns.boxplot(x='rank', y='score', data=df, ax=ax)\n",
    "\n",
    "    ax.set_title('Reward Score vs. Manual Rank')\n",
    "    ax.set_xlabel('Manual Rank (1=Best, 4=Worst)')\n",
    "    ax.set_ylabel('Reward Score')\n",
    "    plt.show()\n",
    "      print(\"\\nAnalysis of the plot:\")\n",
    "    print(\"A successful reward model should show a decreasing trend in scores as the rank increases (from 1 to 4).\")\n",
    "    print(\"This means that higher-quality answers (rank 1) should receive higher scores from the model.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
